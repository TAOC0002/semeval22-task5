model_config:
  multi_task:
    bert_model_name: bert-base-uncased
    training_head_type: classification
    num_labels: 1
    visual_embedding_dim: 2048
    special_visual_initialize: true
    embedding_strategy: plain
    bypass_transformer: false
    output_attentions: false
    output_hidden_states: false
    random_initialize: false
    freeze_base: false
    finetune_lr_multiplier: 1
    # Default points to BERT pooler strategy which is to take
    # representation of CLS token after passing it through a dense layer
    pooler_strategy: default
    zerobias: true
    biasfill: 0.01
    losses:
    - type: bce_with_logits
