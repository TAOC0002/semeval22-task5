{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236651aa",
   "metadata": {},
   "source": [
    "# Prepare for ensemble learning (neural network) - train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932583ef",
   "metadata": {},
   "source": [
    "## misogynous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65df1b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:12:17.195737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:12:17.195760: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_misogynous/best.ckpt\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_misogynous.jsonl\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_misogynous/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf_cli.run: \u001b[0mUsing seed 24596794\n",
      "\u001b[32m2022-01-28T09:12:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:12:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:12:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:12:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:12:43 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:12:58 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:12:58 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:12:58 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:12:58 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1300\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1300\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:12:58 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.23it/s]\n",
      "\u001b[32m2022-01-28T09:13:05 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_concat_bert_24596794.csv\n",
      "\u001b[32m2022-01-28T09:13:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "home = \"/home/taochen\"\n",
    "os.chdir(home)\n",
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_misogynous/best.ckpt\")\n",
    "feats_dir = os.path.join(home, \"new_features\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60be5161",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:13:07.295332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:13:07.295354: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.transformer) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.embeddings) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmf_transformer/configs/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option model to mmf_transformer\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmf_transformer_misogynous/best.ckpt\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_misogynous.jsonl\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmf_transformer/configs/mami/defaults.yaml', 'model=mmf_transformer', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmf_transformer_misogynous/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf_cli.run: \u001b[0mUsing seed 9917227\n",
      "\u001b[32m2022-01-28T09:13:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:13:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:13:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:13:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:13:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:13:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:13:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:13:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:13:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:13:37 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  5.03it/s]\n",
      "\u001b[32m2022-01-28T09:13:43 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_mmf_transformer_9917227.csv\n",
      "\u001b[32m2022-01-28T09:13:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_misogynous/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdb3720",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:13:45.478025: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:13:45.478048: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/mami/from_coco.yaml\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/lr_testing/misogynous_individual/best.ckpt\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_misogynous.jsonl\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:13:47 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/mami/from_coco.yaml', 'model=visual_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/lr_testing/misogynous_individual/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf_cli.run: \u001b[0mUsing seed 48077703\n",
      "\u001b[32m2022-01-28T09:13:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:13:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2022-01-28T09:14:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:14:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:14:00 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:14:09 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:14:09 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\r\n",
      "\u001b[32m2022-01-28T09:14:09 | mmf.common.test_reporter: \u001b[0mPredicting for mami\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.35it/s]\n",
      "\u001b[32m2022-01-28T09:14:15 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_visual_bert_48077703.csv\n",
      "\u001b[32m2022-01-28T09:14:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"lr_testing/misogynous_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9707015a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-29 14:01:15.222665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-29 14:01:15.222688: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/mami/from_cc.yaml\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/vilbert_misogynous/best.ckpt\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_misogynous.jsonl\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/mami/from_cc.yaml', 'model=vilbert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/vilbert_misogynous/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf_cli.run: \u001b[0mUsing seed 17859703\n",
      "\u001b[32m2022-01-29T14:01:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-29T14:01:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-29T14:01:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-29T14:01:28 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-29T14:01:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-29T14:01:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-29T14:01:29 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 63/63 [00:08<00:00,  7.60it/s]\n",
      "\u001b[32m2022-01-29T14:01:38 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_vilbert_17859703.csv\n",
      "\u001b[32m2022-01-29T14:01:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "feats_dir = os.path.join(home, \"new_features\")\n",
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_misogynous/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa50d7",
   "metadata": {},
   "source": [
    "## shaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b885f459",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:22:38.996998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:22:38.997021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/late_fusion/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option model to late_fusion\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/late_fusion_shaming/best.ckpt\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_shaming.jsonl\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/late_fusion/mami/defaults.yaml', 'model=late_fusion', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/late_fusion_shaming/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf_cli.run: \u001b[0mUsing seed 41538577\n",
      "\u001b[32m2022-01-28T09:22:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:22:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:22:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:22:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:22:55 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:13 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:13 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:23:13 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.47it/s]\n",
      "\u001b[32m2022-01-28T09:23:20 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_late_fusion_41538577.csv\n",
      "\u001b[32m2022-01-28T09:23:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/late_fusion_shaming/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/late_fusion/mami/defaults.yaml\" \\\n",
    "    model=\"late_fusion\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4527e8f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:23:21.682713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:23:21.682737: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/mami/with_features.yaml\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmbt_shaming/best.ckpt\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_shaming.jsonl\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/mami/with_features.yaml', 'model=mmbt', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmbt_shaming/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf_cli.run: \u001b[0mUsing seed 24318548\n",
      "\u001b[32m2022-01-28T09:23:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:23:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:23:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:23:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:23:37 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:47 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:23:47 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:23:47 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  4.64it/s]\n",
      "\u001b[32m2022-01-28T09:23:54 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_mmbt_24318548.csv\n",
      "\u001b[32m2022-01-28T09:23:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmbt_shaming/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmbt/configs/mami/with_features.yaml\" \\\n",
    "    model=\"mmbt\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a23c77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:23:55.969632: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:23:55.969657: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/mami/from_coco.yaml\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/shaming_individual/best.ckpt\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_shaming.jsonl\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/mami/from_coco.yaml', 'model=visual_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/shaming_individual/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf_cli.run: \u001b[0mUsing seed 58445052\n",
      "\u001b[32m2022-01-28T09:23:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:24:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:24:08 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:24:08 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\r\n",
      "\u001b[32m2022-01-28T09:24:08 | mmf.common.test_reporter: \u001b[0mPredicting for mami\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.34it/s]\n",
      "\u001b[32m2022-01-28T09:24:15 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_visual_bert_58445052.csv\n",
      "\u001b[32m2022-01-28T09:24:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/shaming_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2f03b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-29 14:01:39.626424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-29 14:01:39.626446: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/mami/from_cc.yaml\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/vilbert_shaming/best.ckpt\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_shaming.jsonl\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/mami/from_cc.yaml', 'model=vilbert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/vilbert_shaming/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf_cli.run: \u001b[0mUsing seed 42257610\n",
      "\u001b[32m2022-01-29T14:01:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n"
     ]
    }
   ],
   "source": [
    "feats_dir = os.path.join(home, \"new_features\")\n",
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_shaming/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_shaming.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ad5bd",
   "metadata": {},
   "source": [
    "## stereotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc918ee1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:25:21.484136: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:25:21.484159: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/late_fusion/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option model to late_fusion\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/late_fusion_stereotype/best.ckpt\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_stereotype.jsonl\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/late_fusion/mami/defaults.yaml', 'model=late_fusion', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/late_fusion_stereotype/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_stereotype.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf_cli.run: \u001b[0mUsing seed 23986803\n",
      "\u001b[32m2022-01-28T09:25:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:25:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:25:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:25:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:25:37 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:25:53 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:25:53 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:25:53 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:25:53 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1750\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1750\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:25:53 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.49it/s]\n",
      "\u001b[32m2022-01-28T09:26:00 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_late_fusion_23986803.csv\n",
      "\u001b[32m2022-01-28T09:26:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/late_fusion_stereotype/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/late_fusion/mami/defaults.yaml\" \\\n",
    "    model=\"late_fusion\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_stereotype.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf35d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_stereotype/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_stereotype.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/stereotype_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4cd01",
   "metadata": {},
   "source": [
    "## objectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb227c81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:26:01.605704: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:26:01.605726: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_objectification/best.ckpt\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_objectification.jsonl\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:26:03 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_objectification/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_objectification.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf_cli.run: \u001b[0mUsing seed 4079412\n",
      "\u001b[32m2022-01-28T09:26:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:26:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:26:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:26:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:26:17 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:26:32 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:26:32 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:26:32 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:26:32 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1250\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1250\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:26:32 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.50it/s]\n",
      "\u001b[32m2022-01-28T09:26:39 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_concat_bert_4079412.csv\n",
      "\u001b[32m2022-01-28T09:26:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_objectification.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc54f30",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:26:40.746319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:26:40.746345: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.transformer) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.embeddings) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmf_transformer/configs/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option model to mmf_transformer\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmf_transformer_objectification/best.ckpt\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_objectification.jsonl\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmf_transformer/configs/mami/defaults.yaml', 'model=mmf_transformer', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmf_transformer_objectification/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_objectification.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf_cli.run: \u001b[0mUsing seed 43999494\n",
      "\u001b[32m2022-01-28T09:26:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:26:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:26:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:26:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:26:55 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:27:12 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:27:12 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:27:12 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:27:12 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1100\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1100\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 2\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:27:12 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  4.99it/s]\n",
      "\u001b[32m2022-01-28T09:27:19 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_mmf_transformer_43999494.csv\n",
      "\u001b[32m2022-01-28T09:27:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_objectification.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c547fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_objectification.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332016",
   "metadata": {},
   "source": [
    "## violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1450a2fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:28:40.807535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:28:40.807559: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_violence/best.ckpt\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/val_violence.jsonl\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/new_features\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_violence/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_violence.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/new_features', 'env.report_dir=save/new_mami', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf_cli.run: \u001b[0mUsing seed 43214414\n",
      "\u001b[32m2022-01-28T09:28:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:28:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:28:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:28:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:28:56 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:29:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:29:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:29:13 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:29:13 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2200\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2200\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:29:13 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.50it/s]\n",
      "\u001b[32m2022-01-28T09:29:19 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami/mami_concat_bert_43214414.csv\n",
      "\u001b[32m2022-01-28T09:29:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_violence.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c140b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_violence.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_violence.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc9c48",
   "metadata": {},
   "source": [
    "# Prepare for ensemble learning (neural network) - test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814deb2",
   "metadata": {},
   "source": [
    "## misogynous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783c748d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:31:35.605607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:31:35.605632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_misogynous/best.ckpt\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_misogynous/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf_cli.run: \u001b[0mUsing seed 38280111\n",
      "\u001b[32m2022-01-28T09:31:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:31:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:31:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:31:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:31:52 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:32:06 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:32:06 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:32:07 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:32:07 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1300\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1300\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:32:07 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.20it/s]\n",
      "\u001b[32m2022-01-28T09:32:14 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_concat_bert_38280111.csv\n",
      "\u001b[32m2022-01-28T09:32:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "home = \"/home/taochen\"\n",
    "os.chdir(home)\n",
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_misogynous/best.ckpt\")\n",
    "feats_dir = os.path.join(home, \"test_features\")\n",
    "img_dir = os.path.join(home, \"data\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a0fe48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:33:07.361490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:33:07.361512: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.transformer) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.embeddings) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmf_transformer/configs/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option model to mmf_transformer\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmf_transformer_misogynous/best.ckpt\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmf_transformer/configs/mami/defaults.yaml', 'model=mmf_transformer', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmf_transformer_misogynous/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf_cli.run: \u001b[0mUsing seed 9826047\n",
      "\u001b[32m2022-01-28T09:33:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:33:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:33:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:33:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:33:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:33:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:33:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:33:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:33:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:33:37 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  5.02it/s]\n",
      "\u001b[32m2022-01-28T09:33:44 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_mmf_transformer_9826047.csv\n",
      "\u001b[32m2022-01-28T09:33:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_misogynous/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8d9a6c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:34:41.494937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:34:41.494962: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/mami/from_coco.yaml\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/lr_testing/misogynous_individual/best.ckpt\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:34:43 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/mami/from_coco.yaml', 'model=visual_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/lr_testing/misogynous_individual/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf_cli.run: \u001b[0mUsing seed 44043793\n",
      "\u001b[32m2022-01-28T09:34:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:34:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2022-01-28T09:34:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:34:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:34:54 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:35:02 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:35:02 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\r\n",
      "\u001b[32m2022-01-28T09:35:02 | mmf.common.test_reporter: \u001b[0mPredicting for mami\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:15<00:00,  1.00it/s]\n",
      "\u001b[32m2022-01-28T09:35:18 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_visual_bert_44043793.csv\n",
      "\u001b[32m2022-01-28T09:35:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"lr_testing/misogynous_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01989d",
   "metadata": {},
   "source": [
    "## shaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1c7166",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:39:59.138173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:39:59.138197: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/late_fusion/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option model to late_fusion\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/late_fusion_shaming/best.ckpt\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/late_fusion/mami/defaults.yaml', 'model=late_fusion', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/late_fusion_shaming/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf_cli.run: \u001b[0mUsing seed 2104926\n",
      "\u001b[32m2022-01-28T09:40:02 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:40:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:40:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:40:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:40:16 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:40:34 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:40:34 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:40:34 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:40:34 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:40:34 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.51it/s]\n",
      "\u001b[32m2022-01-28T09:40:40 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_late_fusion_2104926.csv\n",
      "\u001b[32m2022-01-28T09:40:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/late_fusion_shaming/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/late_fusion/mami/defaults.yaml\" \\\n",
    "    model=\"late_fusion\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c976cdfa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:40:42.267533: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:40:42.267557: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/mami/with_features.yaml\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmbt_shaming/best.ckpt\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/mami/with_features.yaml', 'model=mmbt', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmbt_shaming/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf_cli.run: \u001b[0mUsing seed 44887053\n",
      "\u001b[32m2022-01-28T09:40:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:40:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:40:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:40:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:40:58 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:08 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:08 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:08 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:08 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1500\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1500\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:41:08 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  4.72it/s]\n",
      "\u001b[32m2022-01-28T09:41:14 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_mmbt_44887053.csv\n",
      "\u001b[32m2022-01-28T09:41:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmbt_shaming/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmbt/configs/mami/with_features.yaml\" \\\n",
    "    model=\"mmbt\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec3fa76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:41:16.383336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:41:16.383359: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/mami/from_coco.yaml\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/shaming_individual/best.ckpt\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/mami/from_coco.yaml', 'model=visual_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/shaming_individual/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf_cli.run: \u001b[0mUsing seed 18844146\n",
      "\u001b[32m2022-01-28T09:41:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:41:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2022-01-28T09:41:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:41:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:41:28 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:41:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_ids from model.bert.embeddings.position_ids\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\r\n",
      "\u001b[32m2022-01-28T09:41:38 | mmf.common.test_reporter: \u001b[0mPredicting for mami\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.34it/s]\n",
      "\u001b[32m2022-01-28T09:41:45 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_visual_bert_18844146.csv\n",
      "\u001b[32m2022-01-28T09:41:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/shaming_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc474c4",
   "metadata": {},
   "source": [
    "## stereotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c55936",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:44:01.692369: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:44:01.692393: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/late_fusion/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option model to late_fusion\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/late_fusion_stereotype/best.ckpt\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/late_fusion/mami/defaults.yaml', 'model=late_fusion', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/late_fusion_stereotype/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf_cli.run: \u001b[0mUsing seed 4214656\n",
      "\u001b[32m2022-01-28T09:44:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:44:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:44:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:44:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:44:18 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:44:34 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:44:34 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:44:34 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:44:34 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1750\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1750\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:44:34 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.51it/s]\n",
      "\u001b[32m2022-01-28T09:44:40 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_late_fusion_4214656.csv\n",
      "\u001b[32m2022-01-28T09:44:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/late_fusion_stereotype/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/late_fusion/mami/defaults.yaml\" \\\n",
    "    model=\"late_fusion\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_stereotype/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/stereotype_individual/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/visual_bert/configs/mami/from_coco.yaml\" \\\n",
    "    model=\"visual_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352c54d",
   "metadata": {},
   "source": [
    "## objectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c2dde7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:45:56.435811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:45:56.435835: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_objectification/best.ckpt\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_objectification/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf_cli.run: \u001b[0mUsing seed 58865303\n",
      "\u001b[32m2022-01-28T09:45:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:46:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:46:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:46:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:46:12 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:46:27 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:46:27 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:46:27 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:46:27 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1250\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1250\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:46:27 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.50it/s]\n",
      "\u001b[32m2022-01-28T09:46:34 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_concat_bert_58865303.csv\n",
      "\u001b[32m2022-01-28T09:46:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c832f60",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:46:35.700299: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:46:35.700322: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.transformer) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (backend.embeddings) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmf_transformer/configs/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option model to mmf_transformer\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/mmf_transformer_objectification/best.ckpt\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmf_transformer/configs/mami/defaults.yaml', 'model=mmf_transformer', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/mmf_transformer_objectification/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf_cli.run: \u001b[0mUsing seed 38238320\n",
      "\u001b[32m2022-01-28T09:46:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:46:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:46:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:46:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:46:49 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:47:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:47:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:47:07 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:47:07 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1100\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1100\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 2\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:47:07 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 32/32 [00:06<00:00,  5.04it/s]\n",
      "\u001b[32m2022-01-28T09:47:13 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_mmf_transformer_38238320.csv\n",
      "\u001b[32m2022-01-28T09:47:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbeb25ee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 10:58:39.731406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-30 10:58:39.731431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/late_fusion/mami/defaults.yaml\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option model to late_fusion\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/late_fusion_objectification/best.ckpt\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/late_fusion/mami/defaults.yaml', 'model=late_fusion', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/late_fusion_objectification/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf_cli.run: \u001b[0mUsing seed 42201299\n",
      "\u001b[32m2022-01-30T10:58:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-30T10:58:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-30T10:58:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-30T10:58:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-30T10:58:59 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-30T10:59:15 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-30T10:59:15 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-30T10:59:15 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-30T10:59:15 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1600\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1600\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-30T10:59:15 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.14it/s]\n",
      "\u001b[32m2022-01-30T10:59:23 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_late_fusion_42201299.csv\n",
      "\u001b[32m2022-01-30T10:59:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_objectification/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8413a47",
   "metadata": {},
   "source": [
    "## violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19127aee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 09:48:05.060476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 09:48:05.060502: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/others/concat_bert/mami/defaults.yaml\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_bert\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option datasets to mami\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/taochen/new_model_testing/concat_bert_violence/best.ckpt\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.annotations.test[0] to mami/defaults/annotations/test.jsonl\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.features.test[0] to /home/taochen/test_features\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.mami.images.test[0] to /home/taochen/data\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option env.report_dir to save/new_mami_submission\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/others/concat_bert/mami/defaults.yaml', 'model=concat_bert', 'dataset=mami', 'run_type=test', 'checkpoint.resume_file=/home/taochen/new_model_testing/concat_bert_violence/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl', 'dataset_config.mami.features.test[0]=/home/taochen/test_features', 'dataset_config.mami.images.test[0]=/home/taochen/data', 'env.report_dir=save/new_mami_submission', 'evaluation.predict=true'])\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.utils.general: \u001b[0mCUDA Device 0 is: GeForce RTX 2080 Ti\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf_cli.run: \u001b[0mUsing seed 7543730\n",
      "\u001b[32m2022-01-28T09:48:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2022-01-28T09:48:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2022-01-28T09:48:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-01-28T09:48:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-01-28T09:48:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:48:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:48:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:48:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-01-28T09:48:37 | py.warnings: \u001b[0m/home/yewlee/miniconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2200\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2200\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
      "\u001b[32m2022-01-28T09:48:37 | mmf.common.test_reporter: \u001b[0mPredicting for mami\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.47it/s]\n",
      "\u001b[32m2022-01-28T09:48:43 | mmf.common.test_reporter: \u001b[0mWrote predictions for mami to /home/taochen/save/new_mami_submission/mami_concat_bert_7543730.csv\n",
      "\u001b[32m2022-01-28T09:48:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826da923",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/mmf_transformer_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/mmf_transformer/configs/mami/defaults.yaml\" \\\n",
    "    model=\"mmf_transformer\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(home, \"new_model_testing/vilbert_violence/best.ckpt\")\n",
    "\n",
    "!mmf_predict config=\"projects/vilbert/configs/mami/from_cc.yaml\" \\\n",
    "    model=\"vilbert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/test.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    dataset_config.mami.images.test[0]=$img_dir \\\n",
    "    env.report_dir=save/new_mami_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = \"/home/taochen\"\n",
    "os.chdir(home)\n",
    "ckpt_dir = os.path.join(home, \"new_model_testing/concat_bert_misogynous/best.ckpt\")\n",
    "feats_dir = os.path.join(home, \"new_features\")\n",
    "\n",
    "!mmf_predict config=\"projects/others/concat_bert/mami/defaults.yaml\" \\\n",
    "    model=\"concat_bert\" \\\n",
    "    dataset=mami \\\n",
    "    run_type=test \\\n",
    "    checkpoint.resume_file=$ckpt_dir \\\n",
    "    checkpoint.reset.optimizer=True \\\n",
    "    dataset_config.mami.annotations.test[0]=mami/defaults/annotations/val_misogynous.jsonl \\\n",
    "    dataset_config.mami.features.test[0]=$feats_dir \\\n",
    "    env.report_dir=save/new_mami"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmf",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
